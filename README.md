# iFabric

Project that I would like to be a step towards fully automated telecommunications.

This repo begins as a clone from https://github.com/p4lang/tutorials. 

Big thanks to everyone that took time to listen to my ideas and helped me put them together. Also, Thanks for everyone involved in P4 community, because the materials available gave me the opportunity to learn the technology and start working on this project.

The following document goes as follows:
  1. **Current idea**:
   This section should give a good outline of what's currently in the scope of the project. The idea is still under heavy development and changes quite often so little is set in stone.
  
  2. **What's implemented**:
  What is the current state of the project, it's possibilities and what technologies were used.
  
  3. **Perceived values of the solution**:
  What I think are the values of this project and what made me pursue it.
  
  4. **Possible problems**:
  What I currently see as biggest problems of the solution and how they might be addressed.
  
  5. **Possible extensions**:
  What I see as possible extensions of this project if it gets operational.

## Current idea

In current idea *iFabric* is SDN packet-based network that uses P4 switches for data-plane and reinforcement learning algorithms for control-plane. The *iFabric* can be characterized by: high-performance, resilience, flexibility/ease of configuration and quickness of deployment.

Here's briefly how the current idea supports each of those characteristics. They are explained in greater detail later.
1. **High-performance:**
  The network is based on P4 hardware and small pipeline that should allow great performance.
  
2. **Resilience:**
  The network is able to support any topology we wish to use. Devices connected to the network can be connected in any way they want. This allows to make topology and servers as reduntant as we require.
  
3. **Flexibility of configuration:**
  For backwards-compatibility with the infrastracture, the network can be configured in such a way that its operation closely resembles current networks (*TCP/IP*, *Fibre Channel etc.*) and forwarding is done by using fields contained in standard protocols but it might use brand new protocols that *Operator* can define for his infrastracture.
  
4. **Ease of configuration:**
  The configuration should be generated from as little information as possible. In standard scenarios this can be achieved via templates that would define ready to use protocols and topologies that see most use. The network could be thought of as *Infrastracture as a Code* so that in the future the configuration could be automatically generated by an application that needs network to communicate with other devices. More on that idea in **Possible extensions** . 
  
5. **Quickness of deployment:**
  All processes of deployment should be made as automated as possible, that is, after connecting the hardware and providing high-level configuration the network will need time to train itself which will probably be the longest process. It is still unclear what timespans we could be looking at (minutes, days, weeks, months) but that depends mostly upon ever-improving computing and algorithms. With current State-of-Art I believe few weeks should be feasible but that's what I see as entry point not a ceiling. More on that in **Control-plane** .
  

The core of the idea currently consists of four structures briefly introduced here and expanded upon later:
- **Topology**
- **Configuration**
- **Data-plane**
- **Control-plane**
  

The structures that make the iFabric are presented with provided example of how *iFabric* might implement *IP/MAC* network. Later, I show other sample *iFabric* network implementation to highlight the flexibility of configuration.

### 1. Topology:

Topology is made out of *P4* switches, SDN controllers and *Nodes* connected to them. On physical level, we can currently distinguish two types of links: switch-interconnections and *Nodes* to switches connections. For *Node* it means that, it can be connected to any switchport on any switch with as many links per switch as it needs. 

Nodes we might have in our first example could be:
- *email_server*
- *database_server*
- *firewall*
- *website_1*
- *website_dr_1*
- *website_dr_2*
  
Nodes can be put into Groups. We should be able to put Node into many groups at once for example:
1. Websites:
    - *website_1*
    - *website_dr_1*
    - *website_dr_2*
2. DR_group:
    - *website_dr_1*
    - *website_dr_2*
3. Servers:
    - *email_server*
    - *database_server*

### 2. Configuration:
Before the *iFabric* starts we define two sets:
  - *flows, ( F = {flow_1, flow_2 ... flow_n} )*
  - *policy, ( Pol = {pol_1, pol_2 ... pol_3} )*

Current design doesn't support defining those while the *iFabric* is operational but this is something that could be addressed. More in **Possible extensions**

Flows that we might want to configure in our example *IP/MAC* network could look like:
*F = {flow_to_firewall_ip, flow_to_website1, flow_to_DR ...}*

Policies we could define in this network might be:

*Pol = {route_traffic_to_firewall, route_traffic_to_website1, route_traffic_to_DR ...}*

#### Flow - definition:
One flow has a name, priority and set of *Protocol Fields*: 

*ProtF_Set = {ProtF_1, ..., ProtF_n}*

and set of ranges for protocol fields: 

*ProtF_Ranges(ProtF_x) = {(ProtF_Low1, ProtF_High1), (ProtF_Low2, ProtF_High2) ... (ProtF_LowN, ProtF_HighN)} for ProtF_x from ProtF_Set and ProtF_LowN <= ProtF_HighN*

Name is used for easier recognition of flows.

Priority is used for determining which flow we classify traffic as if traffic matches multiple flows.

*Protocol Field* is any data field that we can define using P4. For our *IP/MAC* network we could go with:
```
typedef bit<48> macAddr_t;

header Ethernet_t {
    macAddr_t dstAddr;
    macAddr_t srcAddr;
    bit<16>   etherType;
}


typedef bit<32> ip4Addr_t;

header IPv4_t {
    bit<4>    version;
    bit<4>    ihl;
    bit<8>    diffserv;
    bit<16>   totalLen;
    bit<16>   identification;
    bit<3>    flags;
    bit<13>   fragOffset;
    bit<8>    ttl;
    bit<8>    protocol;
    bit<16>   hdrChecksum;
    ip4Addr_t srcAddr;
    ip4Addr_t dstAddr;
}
```
In this example *Protocol Fields* we could use to define the *flow_to_firewall_ip* could be *IPv4.dstAddr* .
Ranges we could define as *ProtF_Ranges(IPv4.dstAddr) = { (10.0.0.1, 10.0.0.2) }* 

In practice that would mean: **Treat every traffic seen on the network with destination IPv4 address of 10.0.0.1 and 10.0.0.2 as flow called _flow_to_firwall_ip_**
  
For *flow_to_DR* we could use *IPv4.protocol* and *Ethernet.dstAddr* like this:
*ProtF_Ranges(Ethernet.dstAddr) = { (AB:AB:AB:AB:00:01, AB:AB:AB:AB:00:10), (AB:AB:AB:AB:01:01, AB:AB:AB:AB:01:10) }*
*ProtF_Ranges(IPv4.protocol) = { (11,11) }*

In practice that would mean: **Every traffic seen on the network with:**
- **destination MAC address of AB:AB:AB:AB:00:01 to AB:AB:AB:AB:00:10 and IPv4 protocol field of 11**
- **destination MAC address of AB:AB:AB:AB:01:01 to AB:AB:AB:AB:01:10 and IPv4 protocol field of 11**

**is considered a flow called *flow_to_DR***

#### Policy - definition

Policy is made out of named entries where each entry uses source and destination:
For source we can use flow, *Node* and *Group*. For destination it's only *Node* and *Group*.

In our example network we could have entries like:
1.  **Source:** *flow_to_firewall_ip*
    **Destination:** *firewall*
    
2.  **Source:** *flow_to_DR*
    **Destination:** *DR_group*

3.  **Source:** *Websites*
    **Destination:** *Servers*

Entries could be explained as:
1.  **I want all traffic classified as *flow_to_firewall_ip* to be transported to where *firwall Node* is connected**
2.  **I want all traffic classified as *flow_to_DR* to be transported to where *Nodes* from *DR_group* are connected**
3.  **I want all traffic that's not classified as any of the defined flows coming from any of the *Nodes* in the *Websites* to be transported to where *Nodes* from *Servers* are connected**

The issue of what it means to have something transported to whole *Group* (should it be all *Nodes*, all their ports, load-balanced) is further elaborated on in **Possible problems**.


This provides explanation of how a sample *IP/MAC* network could operate like. Different example could see us using this configuration:

```
typedef bit<256> server_ID;

header MyServerFarm_t {
    server_ID   dstID;
    server_ID   srcID;
    bit<32>   someServerPolicyField;
    bit<32>   processOnDstServerThatNeedsThisData;
}

```
With this we could have policy that sends traffic with specific *srcID* and *processOnDstServerThatNeedsThisData* servers towards servers that run processes that need this traffic.

In this scenario we would also have to assume that *Nodes* NICs support compliant encapsulation so that iFabric might parse it into flows. 

### 3. Data-plane

Data-plane in current idea only tackles the problem of forwarding but later this should be naturally extendable to things like buffering, security. More in **Possible extensions**.

Forwarding in P4 is done based on multicast-groups which currently have one-to-one mapping with flow_id. Each flow is assigned multicast-group that then has egress ports chosen for each switch in *iFabric*. For faster flow recognition we use special *myTunnel* header on top of everything that is being sent. Currently, the header consists of five fields:
- header_type: used for identifying if we are looking at *myTunnel* header
- next_protocol: used for identifying the next protcol that is encapsulated
- flow_id: field that identifies the flow that we are looking at
- node_id: placeholder field that may be further used by control-plane
- group_id: same as above

In the future, the *iFabric* header will definitely get prunned of unnecessary data and possibly extended to provide more useful information for control-plane control. More in **Possible extensions**.

The pipeline looks as follows:
1. Do parsing based on protocol stack that the *iFabric*  may see.
2. If there is no *myTunnel* header then append one based on flows defined by *Operator*.
3. Count the bytes that we see on ingress for particular port for particular flow.
4. Assign multicast-group that is same as flow_id.
5. Forward out of all ports that control plane decided are appropriate for this flow for this switch.
6. Count the bytes that we see on egress for particular port for particular flow.
7. If the port is connected to *Node* then strip the *myTunnel* header.


### 4. Control-plane
Control plane is an algorithm that, looking at bytes on ingress/egress on all ports on all switches can decide forwarding that should be done.
Current idea is for using reinforcement-learning algorithm that sees whether or not bytes that we see on ingress are visible where they should be on egress. Rewards should be assigned only for following the policy so the algorithm doesn't overfit to unwanted behaviours. If proper forwarding is achieved then more rewards may be introduced for other things we would like our *iFabric* to do. More on that in **Possible extensions**



## What's implemented

Data-plane

## Perceived values of the solution

## Possible problems

## Possible extensions
